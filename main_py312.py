# main_py312.py - Pipeline de scraping consolidÃ© pour Python 3.12
import sys
import os
import time
import json
from datetime import datetime
import hashlib

# Ajouter le chemin des modules
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), 'scrapers')))

try:
    from galxe_scraper_py312 import GalxeScraperPy312
    GALXE_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  Galxe scraper non disponible: {e}")
    GALXE_AVAILABLE = False

# Tentative d'import du scraper Zealy (si disponible)
try:
    from zealy_scraper import ZealyScraper
    ZEALY_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸  Zealy scraper non disponible: {e}")
    ZEALY_AVAILABLE = False

def run_pipeline():
    """
    ExÃ©cute le pipeline complet de scraping pour toutes les sources,
    consolide les donnÃ©es et les sauvegarde.
    """
    print(f"ğŸš€ DÃ©marrage du pipeline de scraping consolidÃ© Python 3.12 - {datetime.now().isoformat()}")
    all_opportunities = []

    # --- 1. Scraper Galxe ---
    if GALXE_AVAILABLE:
        try:
            print("\n--- Source: Galxe ---")
            galxe_scraper = GalxeScraperPy312()
            galxe_quests = galxe_scraper.scrape_galxe_campaigns(pages=1)
            if galxe_quests:
                all_opportunities.extend(galxe_quests)
                print(f"âœ… Galxe: {len(galxe_quests)} opportunitÃ©s rÃ©cupÃ©rÃ©es.")
            else:
                print("âš ï¸ Galxe: Aucune opportunitÃ© rÃ©cupÃ©rÃ©e.")
        except Exception as e:
            print(f"âŒ Erreur lors du scraping de Galxe: {e}")
    else:
        print("\n--- Source: Galxe ---")
        print("âŒ Scraper Galxe non disponible")

    # --- 2. Scraper Zealy ---
    if ZEALY_AVAILABLE:
        try:
            print("\n--- Source: Zealy ---")
            zealy_scraper = ZealyScraper()
            zealy_raw_data = zealy_scraper.fetch_quests(limit=100)
            zealy_quests = zealy_scraper.parse_quests(zealy_raw_data)
            if zealy_quests:
                all_opportunities.extend(zealy_quests)
                print(f"âœ… Zealy: {len(zealy_quests)} opportunitÃ©s rÃ©cupÃ©rÃ©es.")
            else:
                print("âš ï¸ Zealy: Aucune opportunitÃ© rÃ©cupÃ©rÃ©e.")
        except Exception as e:
            print(f"âŒ Erreur lors du scraping de Zealy: {e}")
    else:
        print("\n--- Source: Zealy ---")
        print("âŒ Scraper Zealy non disponible (dÃ©pendances manquantes)")

    # --- 3. Traitement et Sauvegarde ---
    if not all_opportunities:
        print("\nâš ï¸ Aucune opportunitÃ© n'a Ã©tÃ© rÃ©cupÃ©rÃ©e au total. Fin du pipeline.")
        return

    # Ajout d'un hash unique pour la dÃ©duplication
    processed_opportunities = []
    for opp in all_opportunities:
        # CrÃ©er une chaÃ®ne de caractÃ¨res unique pour chaque opportunitÃ©
        unique_string = f"{opp.get('source')}-{opp.get('id', '')}-{opp.get('title')}"
        opp['hash'] = hashlib.md5(unique_string.encode()).hexdigest()
        processed_opportunities.append(opp)

    # Sauvegarder les donnÃ©es consolidÃ©es
    timestamp = int(time.time())
    output_dir = "data/opportunities"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    filename = f"{output_dir}/opportunities_{timestamp}.json"
    
    with open(filename, "w", encoding='utf-8') as f:
        json.dump(processed_opportunities, f, indent=2, ensure_ascii=False)
    
    # Statistiques dÃ©taillÃ©es
    galxe_count = len([opp for opp in processed_opportunities if opp.get('source') == 'Galxe'])
    zealy_count = len([opp for opp in processed_opportunities if opp.get('source') == 'Zealy'])
    
    print(f"\nğŸ‰ Pipeline terminÃ© avec succÃ¨s!")
    print(f"ğŸ“Š Galxe: {galxe_count} opportunitÃ©s")
    print(f"ğŸ“Š Zealy: {zealy_count} opportunitÃ©s")
    print(f"ğŸ’¾ {len(processed_opportunities)} opportunitÃ©s totales sauvegardÃ©es dans {filename}")
    
    # Afficher quelques exemples
    if processed_opportunities:
        print(f"\nğŸ¯ Exemples d'opportunitÃ©s rÃ©cupÃ©rÃ©es:")
        for i, opp in enumerate(processed_opportunities[:5]):
            title = opp.get('title', 'Sans titre')[:60]
            source = opp.get('source', 'Inconnu')
            print(f"  {i+1}. [{source}] {title}...")

def run_single_test():
    """Test rapide avec une seule page"""
    print("ğŸ§ª Test rapide - Une page seulement\n")
    
    if GALXE_AVAILABLE:
        print("--- Test Galxe ---")
        try:
            galxe_scraper = GalxeScraperPy312()
            quests = galxe_scraper.scrape_page_with_playwright("https://galxe.com/explore?page=1")
            print(f"âœ… Test rÃ©ussi: {len(quests)} quÃªtes trouvÃ©es")
            
            if quests:
                print("ğŸ¯ PremiÃ¨res quÃªtes:")
                for i, quest in enumerate(quests[:3]):
                    print(f"  {i+1}. {quest['title'][:50]}...")
                    print(f"     {quest['link']}")
        except Exception as e:
            print(f"âŒ Test Ã©chouÃ©: {e}")
    else:
        print("âŒ Galxe scraper non disponible")

if __name__ == "__main__":
    try:
        # VÃ©rifier les arguments
        if len(sys.argv) > 1 and sys.argv[1] == "--test":
            run_single_test()
        else:
            run_pipeline()
    except KeyboardInterrupt:
        print("\nğŸ›‘ Pipeline arrÃªtÃ© par l'utilisateur.")
    except Exception as e:
        print(f"\nâŒ Erreur critique: {e}")
        import traceback
        traceback.print_exc()
