# ğŸ—“ï¸ JOUR 4 : Scraper Zealy (API REST)

## ğŸ¯ Objectif Principal
DÃ©velopper un scraper pour l'API REST de Zealy, l'intÃ©grer avec Vault, et le tester complÃ¨tement.

---

## ğŸ“Š RÃ©sumÃ© des TÃ¢ches

| ID | TÃ¢che | PrioritÃ© | DurÃ©e | Status | Tags |
|---|---|---|---|---|---|
| T001 | Structure code Zealy | ğŸ”¥ CRITIQUE | 30min | â³ Ã€ faire | `ğŸ’» DEV` `ğŸŒ WEB3` |
| T002 | Appel API REST | ğŸ”¥ CRITIQUE | 30min | â³ Ã€ faire | `ğŸŒ WEB3` `âš™ï¸ CONFIG` |
| T003 | Parsing donnÃ©es | âš¡ HAUTE | 30min | â³ Ã€ faire | `ğŸ’» DEV` `ğŸ—„ï¸ DATA` |
| T004 | Tests unitaires | âš¡ HAUTE | 30min | â³ Ã€ faire | `ğŸ§ª TEST` |
| T005 | IntÃ©gration complÃ¨te | ğŸ“‹ MOYENNE | 30min | â³ Ã€ faire | `ğŸ’» DEV` `ğŸ”§ DEBUG` |

---

# ğŸ¯ TÃ‚CHE T001 : Structure Code Zealy

## ğŸ¯ MÃ©tadonnÃ©es
- **ID** : T001
- **PrioritÃ©** : ğŸ”¥ CRITIQUE
- **DurÃ©e estimÃ©e** : 30min
- **Statut** : â³ Ã€ faire
- **Tags** : `ğŸ’» DEV` `ğŸŒ WEB3` `ğŸ PYTHON`

## ğŸ“– Contexte

### **Pourquoi** 
Zealy est une plateforme majeure d'engagement Web3 avec des quÃªtes rÃ©munÃ©rÃ©es. Leur API REST est plus simple que GraphQL, parfait pour dÃ©buter l'agrÃ©gation multi-sources.

### **Contexte** 
AprÃ¨s Galxe (GraphQL), Zealy utilise une API REST classique. Plus facile Ã  implÃ©menter, excellente pour valider notre pipeline de parsing standardisÃ©.

### **Impact** 
Sans Zealy, on perd ~30% des opportunitÃ©s quotidiennes. C'est une source majeure pour les early-stage projects.

## ğŸ› ï¸ ExÃ©cution

### **Quoi faire**
CrÃ©er le fichier `scrapers/zealy_scraper.py` avec la classe `ZealyScraper` suivant le mÃªme pattern que Galxe.

### **Comment**
1. **CrÃ©er la structure** :
   ```bash
   # Dans le dossier scrapers/
   touch zealy_scraper.py
   ```
2. **ImplÃ©menter la classe de base** :
   ```python
   import requests
   import json
   from vault_manager import VaultManager

   class ZealyScraper:
       API_BASE = "https://zealy.io/api/v1"
       
       def __init__(self):
           self.vault = VaultManager()
           self.headers = self._get_auth_headers()
           
       def _get_auth_headers(self):
           secrets = self.vault.retrieve_secret("zealy/api")
           return {
               "X-API-Key": secrets["api_key"],
               "Content-Type": "application/json"
           }
   ```
3. **Valider la structure** :
   ```bash
   python -c "from scrapers.zealy_scraper import ZealyScraper; print('âœ… Classe crÃ©Ã©e')"
   ```

### **Outils requis**
- **Python 3.8+** : Langage principal
- **requests** : HTTP client
- **vault_manager** : Gestion des secrets

### **CritÃ¨res de succÃ¨s**
- [ ] Fichier `zealy_scraper.py` crÃ©Ã©
- [ ] Classe `ZealyScraper` importable
- [ ] IntÃ©gration Vault fonctionnelle
- [ ] Headers d'authentification configurÃ©s

## ğŸ”— Relations

### **PrÃ©requis**
- Vault opÃ©rationnel (Jour 1-2)
- vault_manager.py fonctionnel
- Secrets Zealy stockÃ©s dans Vault

### **DÃ©pendants**
- T002 (Appel API) dÃ©pend de cette structure
- T005 (IntÃ©gration) utilise cette classe

### **RÃ©fÃ©rences**
- Discussions DeepSeek : lignes 126-131 du fichier User_Deepseek_Discuss.txt
- Pattern Ã©tabli avec GalxeScraper (Jour 3)

## ğŸ“‹ Suivi

### **Prochaines Ã©tapes**
ImplÃ©menter la mÃ©thode `fetch_quests()` pour rÃ©cupÃ©rer les donnÃ©es via l'API REST.

### **Points de validation**
- Import sans erreur
- Connexion Vault rÃ©ussie
- Headers correctement formatÃ©s

### **Risques identifiÃ©s**
- **Risque 1** : ClÃ© API Zealy invalide â†’ **Mitigation** : Tester avec curl d'abord
- **Risque 2** : Rate limiting â†’ **Mitigation** : Ajouter retry logic

---

# ğŸ¯ TÃ‚CHE T002 : Appel API REST

## ğŸ¯ MÃ©tadonnÃ©es
- **ID** : T002
- **PrioritÃ©** : ğŸ”¥ CRITIQUE
- **DurÃ©e estimÃ©e** : 30min
- **Statut** : â³ Ã€ faire
- **Tags** : `ğŸŒ WEB3` `âš™ï¸ CONFIG` `ğŸ PYTHON`

## ğŸ“– Contexte

### **Pourquoi** 
L'API Zealy est le point d'entrÃ©e pour rÃ©cupÃ©rer les quÃªtes disponibles. Contrairement Ã  GraphQL, l'API REST est plus directe et stable.

### **Contexte** 
Zealy expose ses quÃªtes via `GET /quests` avec pagination. Format de rÃ©ponse JSON classique avec mÃ©tadonnÃ©es.

### **Impact** 
Sans l'appel API, impossible de rÃ©cupÃ©rer les donnÃ©es. C'est le cÅ“ur du scraper.

## ğŸ› ï¸ ExÃ©cution

### **Quoi faire**
ImplÃ©menter la mÃ©thode `fetch_quests()` pour rÃ©cupÃ©rer les quÃªtes via l'endpoint REST de Zealy.

### **Comment**
1. **Ajouter la mÃ©thode Ã  la classe** :
   ```python
   def fetch_quests(self, limit=50, status="active"):
       """RÃ©cupÃ¨re les quÃªtes Zealy via API REST"""
       endpoint = f"{self.API_BASE}/quests"
       params = {
           "limit": limit,
           "status": status,
           "sort": "created_at:desc"
       }
       
       try:
           response = requests.get(
               endpoint,
               headers=self.headers,
               params=params,
               timeout=10
           )
           response.raise_for_status()
           return response.json()
       except Exception as e:
           print(f"Erreur API Zealy: {str(e)}")
           return None
   ```
2. **Test de connexion** :
   ```bash
   python -c "
   from scrapers.zealy_scraper import ZealyScraper
   s = ZealyScraper()
   print('Status:', s.fetch_quests(limit=1))
   "
   ```
3. **Validation rÃ©ponse** :
   - VÃ©rifier le format JSON retournÃ©
   - Identifier les champs importants : `id`, `title`, `reward`, `deadline`

### **Outils requis**
- **requests** : Client HTTP
- **Zealy API Key** : Authentification

### **CritÃ¨res de succÃ¨s**
- [ ] MÃ©thode `fetch_quests()` retourne des donnÃ©es JSON
- [ ] Gestion d'erreurs implÃ©mentÃ©e
- [ ] Timeout configurÃ© (10s max)
- [ ] Status code 200 confirmÃ©

## ğŸ”— Relations

### **PrÃ©requis**
- T001 (Structure code) terminÃ©
- ClÃ© API Zealy valide dans Vault

### **DÃ©pendants**
- T003 (Parsing) utilisera ces donnÃ©es
- T004 (Tests) validera cette mÃ©thode

### **RÃ©fÃ©rences**
- Documentation API Zealy
- Pattern GalxeScraper pour la gestion d'erreurs

## ğŸ“‹ Suivi

### **Prochaines Ã©tapes**
Parser les donnÃ©es JSON retournÃ©es pour les normaliser.

### **Points de validation**
- RÃ©ponse API non-null
- Format JSON valide
- Presence des champs attendus

### **Risques identifiÃ©s**
- **Risque 1** : Rate limiting (429) â†’ **Mitigation** : Exponential backoff
- **Risque 2** : API down â†’ **Mitigation** : Fallback vers cache

---

# ğŸ¯ TÃ‚CHE T003 : Parsing DonnÃ©es

## ğŸ¯ MÃ©tadonnÃ©es
- **ID** : T003
- **PrioritÃ©** : âš¡ HAUTE
- **DurÃ©e estimÃ©e** : 30min
- **Statut** : â³ Ã€ faire
- **Tags** : `ğŸ’» DEV` `ğŸ—„ï¸ DATA` `ğŸ PYTHON`

## ğŸ“– Contexte

### **Pourquoi** 
Les donnÃ©es brutes de l'API doivent Ãªtre normalisÃ©es dans un format standard commun avec Galxe pour la suite du pipeline.

### **Contexte** 
Zealy retourne un format diffÃ©rent de Galxe. Il faut extraire et normaliser : titre, rÃ©compense, temps estimÃ©, deadline.

### **Impact** 
Sans parsing cohÃ©rent, impossible de calculer le ROI et de filtrer efficacement.

## ğŸ› ï¸ ExÃ©cution

### **Quoi faire**
CrÃ©er la mÃ©thode `parse_quests()` qui transforme la rÃ©ponse API en format standardisÃ©.

### **Comment**
1. **Analyser la structure de rÃ©ponse Zealy** :
   ```json
   {
     "data": [
       {
         "id": "quest_123",
         "title": "Complete Twitter Follow",
         "description": "Follow our Twitter account",
         "xp_reward": 100,
         "estimated_time": 2,
         "deadline": "2024-01-31T23:59:59Z",
         "status": "active"
       }
     ]
   }
   ```
2. **ImplÃ©menter le parsing** :
   ```python
   def parse_quests(self, raw_data):
       """Transforme la rÃ©ponse Zealy en format standard"""
       if not raw_data or "data" not in raw_data:
           return []
       
       quests = []
       for quest in raw_data["data"]:
           quests.append({
               "id": quest["id"],
               "title": quest["title"],
               "description": quest.get("description", ""),
               "reward": f"{quest['xp_reward']} XP",
               "estimated_time": quest.get("estimated_time", 5),
               "deadline": quest.get("deadline"),
               "source": "Zealy"
           })
       
       return quests
   ```
3. **Test du parsing** :
   ```python
   # Test avec donnÃ©es mockÃ©es
   sample_data = {"data": [{"id": "test", "title": "Test Quest", "xp_reward": 50}]}
   parsed = scraper.parse_quests(sample_data)
   assert len(parsed) == 1
   assert parsed[0]["source"] == "Zealy"
   ```

### **Outils requis**
- **Python JSON** : Manipulation donnÃ©es
- **DonnÃ©es de test** : Validation parsing

### **CritÃ¨res de succÃ¨s**
- [ ] MÃ©thode `parse_quests()` crÃ©Ã©e
- [ ] Format de sortie standardisÃ©
- [ ] Gestion des champs manquants
- [ ] Source "Zealy" ajoutÃ©e

## ğŸ”— Relations

### **PrÃ©requis**
- T002 (Appel API) doit fonctionner
- Structure de donnÃ©es API connue

### **DÃ©pendants**
- Calcul ROI (Jour 6) utilisera ces donnÃ©es normalisÃ©es
- Tests d'intÃ©gration valideront ce format

### **RÃ©fÃ©rences**
- Format GalxeScraper pour cohÃ©rence
- Pipeline ETL global du projet

## ğŸ“‹ Suivi

### **Prochaines Ã©tapes**
Ã‰crire les tests unitaires pour valider le parsing.

### **Points de validation**
- Format cohÃ©rent avec autres scrapers
- Gestion des erreurs de parsing
- Champs obligatoires prÃ©sents

### **Risques identifiÃ©s**
- **Risque 1** : Structure API change â†’ **Mitigation** : Validation schema
- **Risque 2** : Champs manquants â†’ **Mitigation** : Valeurs par dÃ©faut

---

# ğŸ¯ TÃ‚CHE T004 : Tests Unitaires

## ğŸ¯ MÃ©tadonnÃ©es
- **ID** : T004
- **PrioritÃ©** : âš¡ HAUTE
- **DurÃ©e estimÃ©e** : 30min
- **Statut** : â³ Ã€ faire
- **Tags** : `ğŸ§ª TEST` `ğŸ’» DEV` `ğŸ PYTHON`

## ğŸ“– Contexte

### **Pourquoi** 
Les tests garantissent la fiabilitÃ© du scraper Zealy et permettent de dÃ©tecter les rÃ©gressions lors des Ã©volutions.

### **Contexte** 
Suite au dÃ©veloppement, il faut valider chaque mÃ©thode avec des donnÃ©es mockÃ©es pour Ã©viter la dÃ©pendance Ã  l'API rÃ©elle.

### **Impact** 
Sans tests, impossible de garantir la stabilitÃ© lors des dÃ©ploiements automatiques.

## ğŸ› ï¸ ExÃ©cution

### **Quoi faire**
CrÃ©er le fichier `tests/test_zealy_scraper.py` avec les tests pour toutes les mÃ©thodes dÃ©veloppÃ©es.

### **Comment**
1. **CrÃ©er le fichier de tests** :
   ```bash
   touch tests/test_zealy_scraper.py
   ```
2. **ImplÃ©menter les tests** :
   ```python
   import unittest
   from unittest.mock import patch, Mock
   from scrapers.zealy_scraper import ZealyScraper

   class TestZealyScraper(unittest.TestCase):
       
       @patch('scrapers.zealy_scraper.requests.get')
       def test_fetch_quests_success(self, mock_get):
           # Mock successful API response
           mock_response = Mock()
           mock_response.status_code = 200
           mock_response.json.return_value = {
               "data": [{
                   "id": "test_quest",
                   "title": "Test Quest",
                   "xp_reward": 100,
                   "estimated_time": 3
               }]
           }
           mock_get.return_value = mock_response
           
           scraper = ZealyScraper()
           result = scraper.fetch_quests(limit=1)
           
           self.assertIsNotNone(result)
           self.assertIn("data", result)

       @patch('scrapers.zealy_scraper.requests.get')
       def test_fetch_quests_failure(self, mock_get):
           # Mock API failure
           mock_get.side_effect = Exception("API Error")
           
           scraper = ZealyScraper()
           result = scraper.fetch_quests()
           
           self.assertIsNone(result)

       def test_parse_quests(self):
           scraper = ZealyScraper()
           sample_data = {
               "data": [{
                   "id": "q1",
                   "title": "Test Quest",
                   "xp_reward": 50,
                   "estimated_time": 2
               }]
           }
           
           parsed = scraper.parse_quests(sample_data)
           
           self.assertEqual(len(parsed), 1)
           self.assertEqual(parsed[0]["source"], "Zealy")
           self.assertEqual(parsed[0]["reward"], "50 XP")

   if __name__ == "__main__":
       unittest.main()
   ```
3. **ExÃ©cuter les tests** :
   ```bash
   python -m unittest tests/test_zealy_scraper.py -v
   ```

### **Outils requis**
- **unittest** : Framework de tests Python
- **unittest.mock** : Mock des dÃ©pendances externes

### **CritÃ¨res de succÃ¨s**
- [ ] Tous les tests passent (OK)
- [ ] Couverture des mÃ©thodes principales
- [ ] Tests d'erreur inclus
- [ ] Mock des appels externes

## ğŸ”— Relations

### **PrÃ©requis**
- T001, T002, T003 terminÃ©s
- Structure de test Ã©tablie (dossier tests/)

### **DÃ©pendants**
- CI/CD utilisera ces tests
- Tests d'intÃ©gration (Jour 12) s'appuieront dessus

### **RÃ©fÃ©rences**
- Tests GalxeScraper comme rÃ©fÃ©rence
- Standards de test du projet

## ğŸ“‹ Suivi

### **Prochaines Ã©tapes**
IntÃ©grer le scraper dans le pipeline principal.

### **Points de validation**
- 100% des tests passent
- Couverture de code satisfaisante
- Tests d'erreurs inclus

### **Risques identifiÃ©s**
- **Risque 1** : Faux positifs â†’ **Mitigation** : Tests edge cases
- **Risque 2** : DÃ©pendances externes â†’ **Mitigation** : Mocks complets

---

# ğŸ¯ TÃ‚CHE T005 : IntÃ©gration ComplÃ¨te

## ğŸ¯ MÃ©tadonnÃ©es
- **ID** : T005
- **PrioritÃ©** : ğŸ“‹ MOYENNE
- **DurÃ©e estimÃ©e** : 30min
- **Statut** : â³ Ã€ faire
- **Tags** : `ğŸ’» DEV` `ğŸ”§ DEBUG` `ğŸŒ WEB3`

## ğŸ“– Contexte

### **Pourquoi** 
Il faut intÃ©grer le scraper Zealy dans le script principal `main.py` pour crÃ©er un pipeline multi-sources Galxe + Zealy.

### **Contexte** 
Le script main.py doit maintenant orchestrer les deux scrapers et agrÃ©ger leurs donnÃ©es dans un format cohÃ©rent.

### **Impact** 
Sans intÃ©gration, le scraper Zealy reste isolÃ© et n'apporte pas de valeur au pipeline global.

## ğŸ› ï¸ ExÃ©cution

### **Quoi faire**
Modifier `main.py` pour inclure le scraper Zealy et fusionner les donnÃ©es avec Galxe.

### **Comment**
1. **Mettre Ã  jour main.py** :
   ```python
   from scrapers.galxe_scraper import GalxeScraper
   from scrapers.zealy_scraper import ZealyScraper
   import json
   import time

   def run_all_scrapers():
       print("ğŸš€ DÃ©marrage pipeline multi-sources")
       
       # Initialiser scrapers
       galxe = GalxeScraper()
       zealy = ZealyScraper()
       
       all_opportunities = []
       
       # Scraping Galxe
       print("ğŸ“Š Scraping Galxe...")
       galxe_data = galxe.fetch_quests(limit=50)
       if galxe_data:
           opportunities_galxe = galxe.parse_quests(galxe_data)
           all_opportunities.extend(opportunities_galxe)
           print(f"âœ… {len(opportunities_galxe)} opportunitÃ©s Galxe")
       
       # Scraping Zealy
       print("ğŸ“Š Scraping Zealy...")
       zealy_data = zealy.fetch_quests(limit=50)
       if zealy_data:
           opportunities_zealy = zealy.parse_quests(zealy_data)
           all_opportunities.extend(opportunities_zealy)
           print(f"âœ… {len(opportunities_zealy)} opportunitÃ©s Zealy")
       
       # Sauvegarde agrÃ©gÃ©e
       timestamp = int(time.time())
       filename = f"data/all_opportunities_{timestamp}.json"
       with open(filename, "w") as f:
           json.dump(all_opportunities, f, indent=2)
       
       print(f"ğŸ’¾ {len(all_opportunities)} opportunitÃ©s totales â†’ {filename}")

   if __name__ == "__main__":
       run_all_scrapers()
   ```
2. **Test d'intÃ©gration** :
   ```bash
   python main.py
   ```
3. **Validation du fichier de sortie** :
   ```bash
   # VÃ©rifier que le JSON contient des donnÃ©es des deux sources
   jq '.[] | .source' data/all_opportunities_*.json | sort | uniq
   # Doit afficher : "Galxe" et "Zealy"
   ```

### **Outils requis**
- **JSON** : Manipulation des donnÃ©es
- **jq** : Validation des donnÃ©es (optionnel)

### **CritÃ¨res de succÃ¨s**
- [ ] main.py exÃ©cute sans erreur
- [ ] DonnÃ©es Galxe et Zealy prÃ©sentes
- [ ] Fichier JSON agrÃ©gÃ© crÃ©Ã©
- [ ] Sources distinctes identifiÃ©es

## ğŸ”— Relations

### **PrÃ©requis**
- T001-T004 terminÃ©s
- GalxeScraper fonctionnel (Jour 3)

### **DÃ©pendants**
- Calcul ROI (Jour 6) utilisera ces donnÃ©es agrÃ©gÃ©es
- Dashboard (Jour 9) affichera ces mÃ©triques

### **RÃ©fÃ©rences**
- Pipeline ETL global du projet
- Architecture multi-sources prÃ©vue

## ğŸ“‹ Suivi

### **Prochaines Ã©tapes**
Ajouter les scrapers Twitter/RSS (Jour 5) au pipeline.

### **Points de validation**
- ExÃ©cution complÃ¨te sans erreur
- DonnÃ©es des deux sources prÃ©sentes
- Format JSON cohÃ©rent

### **Risques identifiÃ©s**
- **Risque 1** : Une source Ã©choue, tout s'arrÃªte â†’ **Mitigation** : Gestion d'erreurs par source
- **Risque 2** : Doublons entre sources â†’ **Mitigation** : DÃ©duplication (Jour 6)

---

## ğŸ“ˆ MÃ©triques de la JournÃ©e

### **Planification**
- **Nombre de tÃ¢ches** : 5
- **DurÃ©e totale estimÃ©e** : 2h30
- **PrioritÃ©s** : 2 critiques, 2 hautes, 1 moyenne

### **DÃ©pendances**
- **TÃ¢ches bloquantes** : 2 (T001, T002)
- **TÃ¢ches indÃ©pendantes** : 0 (toutes sÃ©quentielles)

### **Domaines couverts**
- **DÃ©veloppement** : 4 tÃ¢ches
- **Tests** : 1 tÃ¢che
- **Configuration** : 1 tÃ¢che

---

## ğŸ”„ Instructions SpÃ©ciales pour l'Agent

### **Ordre d'exÃ©cution recommandÃ©**
1. T001 (Structure) - Base obligatoire
2. T002 (API) - DÃ©pend de T001
3. T003 (Parsing) - DÃ©pend de T002
4. T004 (Tests) - Validation complÃ¨te
5. T005 (IntÃ©gration) - Finalisation

### **Points d'attention**
- âš ï¸ VÃ©rifier que Vault contient les secrets Zealy avant de commencer
- âš ï¸ Tester chaque Ã©tape avant de passer Ã  la suivante
- âš ï¸ Garder le mÃªme pattern que GalxeScraper pour la cohÃ©rence

### **Ressources utiles**
- Documentation API Zealy : https://docs.zealy.io/api
- Pattern GalxeScraper du Jour 3
- Tests unitaires existants comme rÃ©fÃ©rence

---

## ğŸ¯ Objectifs de Validation

Ã€ la fin de cette journÃ©e, l'agent doit avoir :
- âœ… Un scraper Zealy fonctionnel et testÃ©
- âœ… Une intÃ©gration multi-sources (Galxe + Zealy)
- âœ… Des tests unitaires qui passent
- âœ… Un fichier JSON avec opportunitÃ©s des deux sources
- âœ… Une base solide pour ajouter d'autres sources

---

*CrÃ©Ã© le : 22 juillet 2025*  
*BasÃ© sur : User_Deepseek_Discuss.txt lignes 126-145*  
*Prochaine Ã©tape : Jour 5 - Scrapers Secondaires (Twitter/RSS)*
