# ğŸ—“ï¸ Jour 3 : Scraper Galxe (API GraphQL)

## ğŸ¯ Objectif du Jour
- DÃ©velopper un scraper pour l'API GraphQL de Galxe
- IntÃ©grer l'authentification Vault
- Tester le parsing des donnÃ©es et crÃ©er les tests unitaires

---

## â° CrÃ©neau 1 : 0:00 - 0:30
**TÃ¢che :** Structure du Code  
**Action :**
```python
# CrÃ©er scrapers/galxe_scraper.py
import requests
import json
from vault_manager import VaultManager

class GalxeScraper:
    GRAPHQL_URL = "https://graphigo.prd.galaxy.eco/query"
    
    def __init__(self):
        self.vault = VaultManager()
        self.headers = self._get_auth_headers()
```
**Livrable :** Fichier scrapers/galxe_scraper.py crÃ©Ã©

---

## â° CrÃ©neau 2 : 0:30 - 1:00
**TÃ¢che :** RequÃªte GraphQL  
**Action :**
```python
def fetch_quests(self, space_id="1", limit=50):
    query = """
    query GetQuests($spaceId: ID!, $limit: Int!) {
        space(id: $spaceId) {
            quests(first: $limit) {
                edges {
                    node {
                        id title description reward
                        startTime endTime
                    }
                }
            }
        }
    }
    """
    # ImplÃ©mentation avec gestion d'erreurs
```
**Livrable :** MÃ©thode fetch_quests() fonctionnelle

---

## â° CrÃ©neau 3 : 1:00 - 1:30
**TÃ¢che :** Parsing des DonnÃ©es  
**Action :**
```python
def parse_quests(self, raw_data):
    if not raw_data or "data" not in raw_data:
        return []
    
    quests = []
    edges = raw_data["data"]["space"]["quests"]["edges"]
    
    for edge in edges:
        node = edge["node"]
        quests.append({
            "id": node["id"],
            "title": node["title"],
            "source": "Galxe"
        })
```
**Livrable :** Format de donnÃ©es standardisÃ©

---

## â° CrÃ©neau 4 : 1:30 - 2:00
**TÃ¢che :** Tests Unitaires  
**Action :**
```python
# CrÃ©er tests/test_galxe_scraper.py
import unittest
from unittest.mock import patch
from scrapers.galxe_scraper import GalxeScraper

class TestGalxeScraper(unittest.TestCase):
    @patch('scrapers.galxe_scraper.requests.post')
    def test_fetch_success(self, mock_post):
        # Mock API response et tests
```
**Livrable :** Tests unitaires passants

---

## â° CrÃ©neau 5 : 2:00 - 2:30
**TÃ¢che :** IntÃ©gration ComplÃ¨te  
**Action :**
```python
# CrÃ©er main.py
from scrapers.galxe_scraper import GalxeScraper
import json, time

def run_galxe_scraper():
    scraper = GalxeScraper()
    raw_data = scraper.fetch_quests(limit=100)
    quests = scraper.parse_quests(raw_data)
    
    # Sauvegarde JSON horodatÃ©e
```
**Livrable :** Pipeline Galxe fonctionnel avec export JSON

---

## ğŸ“œ VÃ©rification Finale
- [ ] GalxeScraper classe crÃ©Ã©e et importable
- [ ] IntÃ©gration Vault pour authentification
- [ ] RequÃªte GraphQL fonctionnelle
- [ ] Parsing au format standardisÃ©
- [ ] Tests unitaires > 90% success
- [ ] Export JSON avec timestamp

---

## ğŸš€ Prochaines Ã‰tapes (Jour 4)
- DÃ©veloppement du Scraper Zealy
- IntÃ©gration multi-sources
- Calcul du ROI

---

*Note : Sauvegarder les clÃ©s API Galxe dans Vault via vault_manager.py*
