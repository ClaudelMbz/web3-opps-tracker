import feedparser
import requests
from datetime import datetime
from langdetect import detect, DetectorFactory
import hashlib
import re
import time
from googletrans import Translator

# Assurer des r√©sultats reproductibles pour langdetect
DetectorFactory.seed = 0

class TwitterRSSScraper:
    def __init__(self):
        """Initialise le scraper Twitter RSS avec les flux principaux et fallback"""
        # Flux RSS sp√©cialis√©s en opportunit√©s et airdrops (EN + FR + Twitter)
        self.feeds = [
            # Sources alternatives pour airdrops et opportunit√©s crypto
            "https://forkast.news/feed/",  # Forkast News - crypto news avec opportunit√©s
            "https://cointelegraph.com/rss",  # Cointelegraph - actualit√©s crypto
            "https://decrypt.co/feed",  # Decrypt - Web3 et opportunit√©s
            "https://thedefiant.io/feed/",  # The Defiant - DeFi opportunities
            
            # Sources anglaises traditionnelles
            "https://beincrypto.com/feed/",  # BeInCrypto - couvre les airdrops
            "https://cryptopotato.com/feed/",  # CryptoPotato - opportunit√©s crypto
            "https://www.cryptonews.com/feed/",  # CryptoNews - actualit√©s + opportunit√©s
            
            # Sources fran√ßaises
            "https://cryptonaute.fr/feed/",  # Cryptonaute - actualit√©s crypto FR
            "https://www.cryptoast.fr/feed/"  # Cryptoast - crypto fran√ßais
        ]
        
        # Sources fallback sp√©cialis√©es (sites d'airdrops)
        self.fallback_feeds = [
            "https://airdrops.io/feed/",  # Principal site d'airdrops - 15-20 opp/jour
            "https://coinairdrops.com/feed/",  # Airdrops v√©rifi√©s - 10-15 opp/jour
            "https://cryptoairdrops.com/feed/",  # Alternative - 5-10 opp/jour
            "https://airdropalert.com/feed/",  # Alertes d'airdrops - 10-15 opp/jour
            "https://earnifi.com/feed/",  # Opportunit√©s DeFi - 5-10 opp/jour
            "https://dappradar.com/blog/feed",  # DApp opportunities - 5-8 opp/jour
        ]
        
        # Configuration
        self.timeout = 10
        self.max_retries = 2
        
        # Traducteur Google
        self.translator = Translator()
        self.translation_cache = {}  # Cache pour √©viter les traductions r√©p√©t√©es
        
    def test_connection(self):
        """Test la connectivit√© aux feeds RSS"""
        print("üîç Test de connectivit√© aux flux RSS...")
        
        # Test des flux principaux
        print("\nüì± Flux Twitter RSS:")
        for feed_url in self.feeds:
            self._test_single_feed(feed_url)
            
        # Test des fallbacks
        print("\nüîÑ Flux Fallback:")
        for feed_url in self.fallback_feeds:
            self._test_single_feed(feed_url)
            
    def _test_single_feed(self, feed_url):
        """Test un seul flux RSS"""
        try:
            response = requests.head(feed_url, timeout=self.timeout)
            if response.status_code == 200:
                print(f"‚úÖ {feed_url}: OK ({response.status_code})")
            else:
                print(f"‚ö†Ô∏è {feed_url}: {response.status_code}")
        except requests.exceptions.Timeout:
            print(f"‚è∞ {feed_url}: Timeout")
        except Exception as e:
            print(f"‚ùå {feed_url}: {str(e)}")
            
    def fetch_opportunities(self, max_entries=20):
        """R√©cup√®re les opportunit√©s depuis les flux RSS principaux"""
        print(f"üì° R√©cup√©ration des opportunit√©s RSS (max: {max_entries})...")
        all_entries = []
        
        for feed_url in self.feeds:
            try:
                print(f"üìä Scraping {feed_url}")
                feed = feedparser.parse(feed_url)
                
                if hasattr(feed, 'entries') and feed.entries:
                    for entry in feed.entries[:max_entries]:
                        all_entries.append({
                            'title': entry.title,
                            'link': entry.link,
                            'published': entry.get('published', ''),
                            'summary': entry.get('summary', entry.get('description', '')),
                            'source_feed': feed_url,
                            'is_fallback': False
                        })
                    print(f"  ‚úÖ {len(feed.entries[:max_entries])} entr√©es r√©cup√©r√©es")
                else:
                    print(f"  ‚ö†Ô∏è Aucune entr√©e trouv√©e")
                    
            except Exception as e:
                print(f"  ‚ùå Erreur {feed_url}: {str(e)}")
                continue
                
        print(f"üìä Total: {len(all_entries)} entr√©es RSS principales")
        return all_entries
        
    def fetch_fallback_opportunities(self, max_entries=15):
        """R√©cup√®re depuis les sources fallback"""
        print(f"üîÑ R√©cup√©ration fallback (max: {max_entries})...")
        fallback_entries = []
        
        for feed_url in self.fallback_feeds:
            try:
                print(f"üîÑ Fallback scraping {feed_url}")
                feed = feedparser.parse(feed_url)
                
                if hasattr(feed, 'entries') and feed.entries:
                    for entry in feed.entries[:max_entries]:
                        fallback_entries.append({
                            'title': entry.title,
                            'link': entry.link,
                            'published': entry.get('published', ''),
                            'summary': entry.get('summary', entry.get('description', '')),
                            'source_feed': feed_url,
                            'is_fallback': True
                        })
                    print(f"  ‚úÖ {len(feed.entries[:max_entries])} entr√©es fallback")
                else:
                    print(f"  ‚ö†Ô∏è Aucune entr√©e fallback")
                    
            except Exception as e:
                print(f"  ‚ö†Ô∏è Fallback failed {feed_url}: {str(e)}")
                continue
                
        print(f"üîÑ Total fallback: {len(fallback_entries)} entr√©es")
        return fallback_entries
        
    def fetch_all_opportunities(self, max_entries=15):
        """R√©cup√®re toutes les opportunit√©s (principales + fallback) avec d√©duplication"""
        print(f"üöÄ R√©cup√©ration compl√®te des opportunit√©s (max: {max_entries} par source)...")
        
        # R√©cup√©ration des flux principaux
        main_entries = self.fetch_opportunities(max_entries)
        
        # R√©cup√©ration des flux fallback
        fallback_entries = self.fetch_fallback_opportunities(max_entries)
        
        # Combinaison
        all_entries = main_entries + fallback_entries
        
        # D√©duplication bas√©e sur l'URL
        seen_urls = set()
        deduplicated_entries = []
        
        for entry in all_entries:
            if entry['link'] not in seen_urls:
                seen_urls.add(entry['link'])
                deduplicated_entries.append(entry)
            else:
                print(f"üîÑ Duplicate removed: {entry['title'][:30]}...")
        
        print(f"üìä Total apr√®s d√©duplication: {len(deduplicated_entries)} entr√©es uniques")
        return deduplicated_entries
        
    def is_supported_language(self, text):
        """D√©tecte si le contenu est en anglais, fran√ßais OU espagnol"""
        try:
            # Nettoyer le texte
            clean_text = text.replace('\n', ' ').replace('\t', ' ').strip()
            if len(clean_text) < 10:
                return True  # Trop court pour d√©tecter, on garde
            
            detected_lang = detect(clean_text)
            # Accepter l'anglais, le fran√ßais, l'espagnol, le portugais
            # + langues parfois confondues par langdetect (lt=lituanien, ko=cor√©en peuvent √™tre des faux positifs)
            return detected_lang in ['en', 'fr', 'es', 'pt', 'lt']
            
        except Exception as e:
            # En cas d'erreur, on garde le contenu
            return True
            
    def translate_to_english(self, text, source_lang='auto'):
        """Traduit un texte vers l'anglais en utilisant Google Translate"""
        try:
            # V√©rifier le cache
            cache_key = f"{source_lang}:{text[:50]}"
            if cache_key in self.translation_cache:
                return self.translation_cache[cache_key]
            
            # Traduire
            translation = self.translator.translate(text, src=source_lang, dest='en')
            translated_text = translation.text
            
            # Mettre en cache
            self.translation_cache[cache_key] = translated_text
            
            return translated_text
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur de traduction: {str(e)}")
            return text  # Retourner le texte original en cas d'erreur
            
    def _estimate_reward(self, title, summary):
        """Estime la r√©compense bas√©e sur le titre et r√©sum√©"""
        text = f"{title} {summary}".lower()
        
        # Recherche de montants explicites
        amounts = re.findall(r'\$(\d+)', text)
        if amounts:
            return f"${amounts[0]}"
            
        # Recherche de crypto amounts
        crypto_amounts = re.findall(r'(\d+)\s*(usdt|usdc|eth|btc)', text)
        if crypto_amounts:
            amount, token = crypto_amounts[0]
            return f"{amount} {token.upper()}"
        
        # Estimation bas√©e sur des mots-cl√©s
        if any(word in text for word in ['major', 'huge', 'massive', 'big']):
            return "$50-100 estimated"
        elif any(word in text for word in ['medium', 'good', 'decent']):
            return "$20-50 estimated"
        elif any(word in text for word in ['small', 'mini', 'quick']):
            return "$5-20 estimated"
        else:
            return "$10-30 estimated"
            
    def parse_rss_data(self, rss_entries):
        """Transforme les entr√©es RSS en format standard"""
        print(f"üîç Parsing {len(rss_entries)} entr√©es RSS...")
        opportunities = []
        
        for entry in rss_entries:
            try:
                # V√©rification de la langue (anglais, fran√ßais ET espagnol)
                full_text = f"{entry['title']} {entry.get('summary', '')}"
                detected_lang = 'en'  # Par d√©faut
                
                try:
                    detected_lang = detect(full_text)
                except:
                    pass
                
                if not self.is_supported_language(full_text):
                    print(f"üåê Skipped ({detected_lang}): {entry['title'][:50]}...")
                    continue
                
                # Traduction si n√©cessaire (espagnol vers anglais)
                original_title = entry['title']
                original_summary = entry.get('summary', '')
                
                if detected_lang == 'es':
                    print(f"üè™ Traducing from Spanish: {original_title[:30]}...")
                    translated_title = self.translate_to_english(original_title, 'es')
                    translated_summary = self.translate_to_english(original_summary, 'es') if original_summary else ''
                    
                    # Utiliser les versions traduites pour le filtrage
                    title_for_filtering = translated_title
                    summary_for_filtering = translated_summary
                else:
                    title_for_filtering = original_title
                    summary_for_filtering = original_summary
                
                # Filtrage basique des opportunit√©s
                title_lower = title_for_filtering.lower()
                
                # Mots-cl√©s √©largis pour capturer plus d'opportunit√©s (anglais + fran√ßais)
                if entry.get('is_fallback', False):
                    # Fallbacks: mots-cl√©s directs d'opportunit√©s
                    keywords = [
                        # Anglais
                        'airdrop', 'free', 'earn', 'reward', 'giveaway', 'claim',
                        'bonus', 'incentive', 'campaign', 'contest', 'competition',
                        'whitelist', 'presale', 'launch', 'testnet', 'mainnet',
                        # Fran√ßais
                        'gratuit', 'gagner', 'r√©compense', 'cadeau', 'concours',
                        'campagne', 'bonus', 'lancement'
                    ]
                else:
                    # Principaux: mots-cl√©s crypto + opportunit√©s
                    keywords = [
                        # Opportunit√©s directes (anglais)
                        'airdrop', 'quest', 'task', 'reward', 'earn', 'free',
                        'giveaway', 'bonus', 'incentive', 'campaign', 'event',
                        'contest', 'competition', 'opportunity', 'program',
                        # Crypto/DeFi terms (anglais)
                        'defi', 'nft', 'token', 'staking', 'yield', 'farming',
                        'trading', 'swap', 'liquidity', 'bridge', 'layer2',
                        'whitelist', 'presale', 'ido', 'ico', 'launch',
                        'testnet', 'mainnet', 'alpha', 'beta',
                        # Termes fran√ßais
                        'gratuit', 'gagner', 'r√©compense', 'jeton', 'crypto',
                        'blockchain', 'opportunit√©', 'programme', 'concours',
                        'campagne', 'lancement', 'finance', 'investir'
                    ]
                
                if any(keyword in title_lower for keyword in keywords):
                    # Estimation de la r√©compense
                    estimated_reward = self._estimate_reward(entry['title'], entry.get('summary', ''))
                    
                    # G√©n√©ration d'ID unique
                    unique_id = f"rss_{hashlib.md5(entry['link'].encode()).hexdigest()[:8]}"
                    
                    # Source name
                    source_name = 'AirdropsFallback' if entry.get('is_fallback') else 'TwitterRSS'
                    
                    opportunities.append({
                        'id': unique_id,
                        'title': entry['title'],
                        'description': entry.get('summary', ''),
                        'reward': estimated_reward,
                        'estimated_time': 5,  # Valeur par d√©faut en minutes
                        'deadline': None,
                        'source': source_name,
                        'url': entry['link'],
                        'published_date': entry.get('published', ''),
                        'source_feed': entry.get('source_feed', ''),
                        'scraped_at': datetime.now().isoformat()
                    })
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur parsing entry: {str(e)}")
                continue
        
        print(f"üîç Filtered {len(opportunities)} opportunities (EN/FR/ES)")
        return opportunities
        
    def test_language_detection(self):
        """Test la d√©tection de langue"""
        print("üß™ Test de d√©tection de langue...")
        
        test_cases = [
            ("Free airdrop for early adopters", True, "English"),
            ("Airdrop gratuit pour les premiers utilisateurs", True, "French"),
            ("Oportunidad de airdrop gratis para los primeros usuarios que se registren en esta plataforma de criptomonedas", True, "Spanish"),  # Texte plus long
            ("üéÅ NEW AIRDROP ALERT! üí∞", True, "English with emojis")
        ]
        
        for text, expected, description in test_cases:
            result = self.is_supported_language(text)
            
            # Debug: montrer quelle langue est d√©tect√©e
            try:
                detected_lang = detect(text)
                debug_info = f"(detected: {detected_lang})"
            except:
                debug_info = "(detection failed)"
            
            status = "‚úÖ" if result == expected else "‚ùå"
            print(f"{status} {description}: {result} {debug_info} (expected: {expected})")
            
        print("üß™ Language detection tests completed")

# Test du scraper si ex√©cut√© directement
if __name__ == "__main__":
    print("üöÄ Test du Twitter RSS Scraper")
    print("=" * 50)
    
    scraper = TwitterRSSScraper()
    
    # Test de connectivit√©
    scraper.test_connection()
    
    # Test de d√©tection de langue
    print("\n" + "=" * 50)
    scraper.test_language_detection()
    
    # Test de r√©cup√©ration compl√®te (limit√© pour les tests)
    print("\n" + "=" * 50)
    print("üß™ Test du scraping complet avec d√©duplication...")
    
    # Test avec la nouvelle m√©thode compl√®te
    all_entries = scraper.fetch_all_opportunities(max_entries=5)
    if all_entries:
        opportunities = scraper.parse_rss_data(all_entries)
        print(f"\nüéØ Opportunit√©s trouv√©es au total: {len(opportunities)}")
        
        # Analyse par source
        twitter_count = sum(1 for opp in opportunities if opp['source'] == 'TwitterRSS')
        fallback_count = sum(1 for opp in opportunities if opp['source'] == 'AirdropsFallback')
        
        print(f"üìä R√©partition:")
        print(f"  - Twitter/RSS: {twitter_count} opportunit√©s")
        print(f"  - Fallback: {fallback_count} opportunit√©s")
        
        print(f"\nüèÜ Top 5 opportunit√©s:")
        for i, opp in enumerate(opportunities[:5], 1):
            print(f"{i}. {opp['title'][:50]}... | {opp['reward']} | {opp['source']}")
            
        # Estimation du volume quotidien
        daily_estimate = len(opportunities) * 4  # Estimation x4 pour 24h
        print(f"\nüìà Estimation quotidienne: ~{daily_estimate} opportunit√©s/jour")
    else:
        print("‚ö†Ô∏è Aucune opportunit√© trouv√©e")
    
    print("\n‚úÖ Tests termin√©s!")
