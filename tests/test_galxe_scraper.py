# tests/test_galxe_scraper.py
import sys
import os
import time
import json
import pytest
from datetime import datetime

# Ajouter le rÃ©pertoire parent au path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from scrapers.galxe_scraper import GalxeScraperEnhanced

@pytest.fixture
def scraper():
    """Fixture pour crÃ©er une instance du scraper Galxe"""
    return GalxeScraperEnhanced()

def test_scraper_initialization():
    """Test l'initialisation du scraper"""
    print("ğŸ§ª Test 1: Initialisation du scraper")
    
    try:
        scraper = GalxeScraperEnhanced()
        stats = scraper.get_stats()
        
        print(f"âœ… Scraper initialisÃ© avec succÃ¨s")
        print(f"ğŸ“Š Comptes disponibles: {stats['accounts_count']}")
        print(f"ğŸ­ Playwright disponible: {stats['playwright_available']}")
        print(f"ğŸ”„ MÃ©thode optimale: {stats['optimal_method']}")
        print(f"ğŸ“ˆ RequÃªtes aujourd'hui: {stats['daily_requests']}")
        print(f"ğŸ“Š Quota restant: {stats['remaining_quota']}")
        
        return scraper
        
    except Exception as e:
        print(f"âŒ Erreur initialisation: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_single_page_scrape(scraper):
    """Test le scraping d'une page unique"""
    print("\nğŸ§ª Test 2: Scraping d'une page unique")
    
    try:
        test_url = "https://galxe.com/explore?page=1"
        print(f"ğŸŒ URL de test: {test_url}")
        
        start_time = time.time()
        html = scraper.scrape_page(test_url, max_retries=2)
        duration = time.time() - start_time
        
        print(f"âœ… Scraping rÃ©ussi en {duration:.2f}s")
        print(f"ğŸ“„ HTML rÃ©cupÃ©rÃ©: {len(html):,} caractÃ¨res")
        
        # VÃ©rifier le contenu
        html_lower = html.lower()
        galxe_indicators = ["galxe", "campaign", "quest", "explore", "web3"]
        found_indicators = [ind for ind in galxe_indicators if ind in html_lower]
        
        if found_indicators:
            print(f"âœ… Contenu Galxe dÃ©tectÃ©: {found_indicators}")
        else:
            print("âš ï¸  Contenu suspect - vÃ©rifier la page")
        
        return html
        
    except Exception as e:
        print(f"âŒ Erreur scraping: {e}")
        return None

def test_campaign_extraction(scraper):
    """Test l'extraction des campagnes"""
    print("\nğŸ§ª Test 3: Extraction des campagnes")
    
    try:
        print("ğŸš€ DÃ©but du scraping avec extraction")
        results = scraper.scrape_galxe_explore(pages=1)
        
        if results:
            result = results[0]
            print(f"âœ… Extraction terminÃ©e")
            print(f"ğŸ“„ Taille HTML: {result.get('html_size', 0):,} caractÃ¨res")
            print(f"ğŸ“Š Campagnes trouvÃ©es: {result.get('campaigns_found', 0)}")
            print(f"ğŸ”„ MÃ©thode utilisÃ©e: {result.get('method', 'N/A')}")
            
            # Afficher quelques campagnes
            campaigns = result.get('campaigns', [])
            if campaigns:
                print(f"\nğŸ“‹ AperÃ§u des campagnes:")
                for i, campaign in enumerate(campaigns[:3], 1):
                    print(f"  {i}. {campaign.get('title', 'Sans titre')}")
                    if 'url' in campaign:
                        print(f"     ğŸ”— {campaign['url']}")
                    if 'reward' in campaign:
                        print(f"     ğŸ {campaign['reward']} points")
            
            return results
        else:
            print("âŒ Aucun rÃ©sultat")
            return None
        
    except Exception as e:
        print(f"âŒ Erreur extraction: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_fallback_mechanism(scraper):
    """Test le mÃ©canisme de fallback"""
    print("\nğŸ§ª Test 4: MÃ©canisme de fallback")
    
    try:
        # Ã‰tat initial
        initial_method = scraper.get_optimal_method()
        print(f"ğŸ”„ MÃ©thode initiale: {initial_method}")
        
        # Forcer le fallback en simulant un quota Ã©levÃ©
        original_requests = scraper.daily_requests
        scraper.daily_requests = 150  # Seuil critique
        
        print(f"ğŸ”„ Simulation quota Ã©levÃ©: {scraper.daily_requests} requÃªtes")
        
        method = scraper.get_optimal_method()
        print(f"ğŸ¯ MÃ©thode aprÃ¨s simulation: {method}")
        
        if method == "playwright" and scraper.playwright_available:
            print("âœ… Fallback vers Playwright activÃ©")
        elif method == "scraperapi":
            print("âœ… ScraperAPI maintenu")
        else:
            print("âš ï¸  Fallback non dÃ©terminÃ©")
        
        # Restaurer l'Ã©tat original
        scraper.daily_requests = original_requests
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur test fallback: {e}")
        return False

def test_stats_and_persistence(scraper):
    """Test les statistiques et la persistance"""
    print("\nğŸ§ª Test 5: Statistiques et persistance")
    
    try:
        # Obtenir les stats
        stats = scraper.get_stats()
        
        print("ğŸ“Š Statistiques complÃ¨tes:")
        for key, value in stats.items():
            print(f"  {key}: {value}")
        
        # VÃ©rifier la sauvegarde
        if os.path.exists(scraper.stats_file):
            with open(scraper.stats_file, 'r') as f:
                saved_stats = json.load(f)
            print(f"âœ… Fichier stats sauvegardÃ©: {scraper.stats_file}")
            print(f"ğŸ“… Date: {saved_stats.get('date', 'N/A')}")
            print(f"â° Timestamp: {saved_stats.get('timestamp', 'N/A')}")
        else:
            print("âš ï¸  Fichier stats non trouvÃ©")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur test stats: {e}")
        return False

def test_content_validation(scraper):
    """Test la validation du contenu"""
    print("\nğŸ§ª Test 6: Validation du contenu")
    
    try:
        # Test avec du contenu valide
        valid_html = """
        <html>
        <body>
            <div class="campaign-card">
                <h3>Test Galxe Campaign</h3>
                <p>A web3 quest for NFT rewards</p>
            </div>
        </body>
        </html>
        """
        
        is_valid = scraper._validate_content(valid_html)
        print(f"âœ… Contenu valide: {is_valid}")
        
        # Test avec du contenu invalide
        invalid_html = "<html><body>Error 404</body></html>"
        is_invalid = scraper._validate_content(invalid_html)
        print(f"âŒ Contenu invalide: {not is_invalid}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur validation: {e}")
        return False

